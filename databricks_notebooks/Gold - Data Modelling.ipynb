{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27f0f4b8-a26e-4064-a51c-47ac59a720a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold Layer - Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e4dca8a-8847-4edb-ac5d-0c35543fa663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Create Connection to Azure Storage account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e63baa8c-7165-4b70-9352-f4e14b2a4ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"team04sa\"\n",
    "application_id = \"7ab46e7b-cc68-4f3f-9903-9a6bae8e347a\"\n",
    "directory_id = \"b7a954b3-aa07-453e-b8a3-97101aeffcad\"\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", \"6CO8Q~LRNOBGY5V~1UjmhmTdtEQwcbNbiB6ojcaw\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d58a9d-8d6b-476b-a852-5691bf6c67b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2) Read delta format from Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c60a97-ddff-4fcf-8932-07d0254a13be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GOLD_BASE = \"abfss://fooddata@team04sa.dfs.core.windows.net/gold/\"\n",
    "SILVER_PATH = \"abfss://fooddata@team04sa.dfs.core.windows.net/silver/\"\n",
    "\n",
    "silver_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "silver_df.cache()\n",
    "print(\"rows:\", silver_df.count())\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4b3542-c26a-490a-ac65-e11ba903b6e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3) Create DIM Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f80617-a899-4aeb-8631-07dfa3a2f9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Country\n",
    "dim_country = (silver_df\n",
    "               .select(F.col(\"adm0_id\").alias(\"country_id\"),\n",
    "                       F.col(\"adm0_name\").alias(\"country\"))\n",
    "               .distinct()\n",
    "               .withColumn(\"country_sk\", F.monotonically_increasing_id())\n",
    "               .select(\"country_sk\", \"country_id\", \"country\")\n",
    "               )\n",
    "\n",
    "# Product\n",
    "dim_product = (silver_df\n",
    "               .select(F.col(\"cm_id\").alias(\"product_id\"), \n",
    "                       F.col(\"cm_name\").alias(\"product_name\"),\n",
    "                       F.col(\"cm_group\").alias(\"product_group\"))\n",
    "               .distinct()\n",
    "               .withColumn(\"product_sk\", F.monotonically_increasing_id())\n",
    "               .select(\"product_sk\", \"product_id\", \"product_name\", \"product_group\")\n",
    "               )\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "dim_time = (\n",
    "    silver_df\n",
    "    .select(\"date_month\", \"mp_year\", \"mp_month\", \"year_month\")\n",
    "    .distinct()\n",
    "    # Quartal: floor((month-1)/3)+1  => 1..4\n",
    "    .withColumn(\n",
    "        \"qtr\",\n",
    "        F.concat(\n",
    "            F.lit(\"Q\"),\n",
    "            (F.floor((F.col(\"mp_month\").cast(\"int\") - F.lit(1)) / F.lit(3)) + F.lit(1)).cast(\"string\")\n",
    "        )\n",
    "    )\n",
    "    .withColumnRenamed(\"mp_year\", \"year\")\n",
    "    .withColumnRenamed(\"mp_month\", \"month\")\n",
    "    .withColumnRenamed(\"date_month\", \"date\")\n",
    ")\n",
    "\n",
    "# Unit\n",
    "dim_unit = (silver_df\n",
    "            .select(F.col(\"std_unit_group\"))\n",
    "            .distinct()\n",
    "            .withColumn(\"unit_sk\", F.monotonically_increasing_id())\n",
    "            .select(\"unit_sk\", \"std_unit_group\")\n",
    "            )\n",
    "\n",
    "# Market\n",
    "dim_market = (silver_df\n",
    "              .select(\"mkt_name\",\"mkt_id\").distinct()\n",
    "              .withColumn(\"market_sk\", F.monotonically_increasing_id())\n",
    "              .select(\"market_sk\", \"mkt_name\",\"mkt_id\")\n",
    "              )\n",
    "\n",
    "# Write DIMs\n",
    "(dim_country.write.mode(\"overwrite\").format(\"delta\").save(GOLD_BASE+\"dim_country\"))\n",
    "(dim_product.write.mode(\"overwrite\").format(\"delta\").save(GOLD_BASE+\"dim_product\"))\n",
    "(dim_time.write.mode(\"overwrite\").format(\"delta\").save(GOLD_BASE+\"dim_time\"))\n",
    "(dim_unit.write.mode(\"overwrite\").format(\"delta\").save(GOLD_BASE+\"dim_unit\"))\n",
    "(dim_market.write.mode(\"overwrite\").format(\"delta\").option(\"mergeSchema\", \"true\").save(GOLD_BASE+\"dim_market\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold - Data Modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
